# ğŸ­ Enhanced Multi-Company Schedule Scraping - Loveable Deployment

## ğŸ¯ Overview

This deployment adds comprehensive multi-company schedule scraping functionality to ensure **schedule syncing works correctly for every company, avdelning/ort (department/location), and skiftlag (shift team)**. The enhanced system provides enterprise-grade logging, error handling, and monitoring capabilities.

## âœ… What's Been Implemented

### ğŸ¢ Multi-Company Support
- **Volvo**: Teams A, B, C, D with departments (Produktion, Montering, Kvalitet, UnderhÃ¥ll, Logistik)
- **SCA**: Teams RÃ¶d, BlÃ¥, Gul, GrÃ¶n with departments (Massa, Papper, UnderhÃ¥ll, Kvalitet, Logistik)
- **SSAB**: Teams 1, 2, 3, 4, 5 with departments (Masugn, StÃ¥lverk, Varmvalsning, Kallvalsning, UnderhÃ¥ll)
- **Boliden**: Teams Alpha, Beta, Gamma, Delta with departments (Gruva, Anrikning, SmÃ¤ltverk, UnderhÃ¥ll, MiljÃ¶)
- **Barilla**: Teams 1, 2, 3, 4, 5 with departments (Produktion, FÃ¶rpackning, Kvalitet, UnderhÃ¥ll, Lager)
- **Extensible**: Easy to add more companies, teams, and departments

### ğŸ—„ï¸ Enhanced Database Schema
```sql
-- New Tables Created:
- schedules: Main schedule data with company/team/department breakdown
- schedule_sources: Track different scraping sources and configurations  
- scrape_logs: Comprehensive logging of all scraping activities

-- Enhanced Existing Tables:
- companies: Added slug, schedule_url, teams_config, departments_config
- teams: Added department, shift_pattern columns
```

### ğŸ”§ Enhanced Scraping Scripts

#### 1. **Multi-Company Scraper** (`scripts/multi-company-scraper.cjs`)
- Scrapes all companies, teams, and departments in parallel
- Comprehensive error handling per company
- Database logging of all activities
- Performance monitoring and statistics

#### 2. **Enhanced Single Scraper** (`scripts/scrape-upload.cjs`)
- Improved version of original scraper with enhanced logging
- Better error handling and debugging capabilities
- Screenshot capture on failures

#### 3. **Reusable Template** (`scripts/puppeteer-template.js`)
- Modular scraping template for creating new scrapers
- Built-in logging, error handling, and debugging features
- Configurable options for different scraping scenarios

#### 4. **Health Check** (`scripts/health-check.js`)
- Monitor scraping system health
- Database connectivity checks
- Recent activity monitoring

### ğŸ“Š Comprehensive Logging & Monitoring

#### Features:
- âœ… **Step-by-step progress tracking** with emojis for easy reading
- âœ… **Environment validation** to check required variables
- âœ… **Performance timing** for each company and overall process
- âœ… **Error details** with stack traces, timestamps, and context
- âœ… **Automatic screenshot capture** on failures for debugging
- âœ… **Database logging** of all scraping activities
- âœ… **Success/failure statistics** per company, team, and department

#### Example Output:
```
ğŸ­ Starting multi-company schedule scraping...
ğŸ“Š Configuration: { totalCompanies: 5, companies: ['VOLVO', 'SCA', 'SSAB', 'BOLIDEN', 'BARILLA'] }
ğŸ” Validating environment...
ğŸ”„ Syncing companies to database...
âœ… Companies synced successfully

ğŸ¢ Processing company: Volvo (volvo)
ğŸ“„ Creating page for Volvo...
ğŸŒ Navigating to https://skiftschema.se/volvo...
âœ… Navigation completed with status: 200
ğŸ” Waiting for primary button...
âœ… Primary button clicked
ğŸ“Š Waiting for schedule table...
âœ… Schedule table loaded
ğŸ“‹ Extracting schedule data...
âœ… Extracted 120 schedule records
ğŸ’¾ Processing schedule data for Volvo...
ğŸ“ˆ Progress: 10/120 records processed
ğŸ“ˆ Progress: 20/120 records processed
...
âœ… Volvo completed: 120 records processed

ğŸ‰ Multi-company scraping completed!
ğŸ“Š Final Results: { totalCompanies: 5, successful: 5, failed: 0, totalRecords: 580 }
```

## ğŸš€ Deployment Steps for Loveable

### 1. Database Setup
```sql
-- Run this SQL in your Supabase dashboard:
-- (Copy from scripts/enhanced-database-schema.sql)
```

### 2. Environment Variables
Add to Loveable environment:
```
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key  
PUPPETEER_EXECUTABLE_PATH=/usr/bin/google-chrome-stable
SCRAPE_FREQUENCY_MINUTES=60
```

### 3. Deploy Files
Upload these files to Loveable:
- `scripts/multi-company-scraper.cjs`
- `scripts/enhanced-database-schema.sql`
- `scripts/puppeteer-template.js`
- `scripts/health-check.js`
- `scripts/README.md`

### 4. Set Up Scheduled Jobs
Configure these cron jobs in Loveable:

**Main Scraping** (Every hour):
```bash
0 * * * * node scripts/multi-company-scraper.cjs
```

**Health Check** (Every 15 minutes):
```bash
*/15 * * * * node scripts/health-check.js
```

**Database Cleanup** (Daily at 2 AM):
```sql
0 2 * * * SELECT cleanup_old_schedules(90);
```

## ğŸ“Š Expected Results

After deployment, the system will:

### âœ… Sync All Companies
- Automatically sync all companies from `data/companies.ts` to database
- Create teams and departments for each company
- Assign proper colors and configurations

### âœ… Scrape All Schedules
- Extract schedule data for every team in every company
- Map teams to appropriate departments
- Handle different schedule formats and structures

### âœ… Comprehensive Logging
- Log every scraping activity to `scrape_logs` table
- Track success/failure rates per company
- Store performance metrics and timing data
- Capture error details for debugging

### âœ… Data Structure
```javascript
// Example schedule record:
{
  id: "uuid",
  company_id: "volvo",
  team_name: "A", 
  department: "Produktion",
  date: "2024-01-15",
  shift_type: "Dagskift",
  start_time: "06:00",
  end_time: "14:00",
  location: "GÃ¶teborg",
  status: "active",
  scraped_at: "2024-01-15T10:30:00Z"
}
```

## ğŸ” Monitoring & Verification

### Database Queries
```sql
-- Check scraping activity
SELECT * FROM scraping_stats ORDER BY last_scrape_time DESC;

-- View current schedules  
SELECT * FROM current_schedules WHERE date >= CURRENT_DATE LIMIT 10;

-- Check error rates
SELECT company_id, 
       COUNT(*) as total_scrapes,
       COUNT(CASE WHEN status = 'error' THEN 1 END) as errors
FROM scrape_logs 
WHERE created_at >= NOW() - INTERVAL '24 hours'
GROUP BY company_id;
```

### Health Check
```bash
# Run health check
npm run scrape:health

# Expected output:
ğŸ¥ Running health check...
Health check results: {
  timestamp: "2024-01-15T10:30:00.000Z",
  database: "âœ… Connected", 
  recentActivity: 5,
  status: "healthy"
}
```

## ğŸ¯ Key Benefits

### ğŸ¢ Complete Company Coverage
- **Every company** gets its schedules scraped
- **Every team** within each company is tracked
- **Every department** is properly mapped and organized

### ğŸ“Š Enterprise-Grade Monitoring
- Real-time progress tracking
- Comprehensive error logging  
- Performance metrics and statistics
- Automatic failure detection and alerts

### ğŸ›¡ï¸ Robust Error Handling
- Individual company failures don't stop the entire process
- Automatic screenshot capture for debugging
- Detailed error context and stack traces
- Graceful recovery and resource cleanup

### âš¡ High Performance
- Parallel processing capabilities
- Request interception for faster scraping
- Efficient database operations with upserts
- Configurable timeouts and retry logic

## ğŸš¨ Important Notes

1. **Very Important**: This ensures schedule syncing works for **every company, avdelning/ort, and skiftlag** as requested
2. **Database Migration**: Run the schema carefully in production
3. **Resource Monitoring**: Monitor memory/CPU usage during scraping
4. **Rate Limiting**: Be respectful of source website limits
5. **Error Alerts**: Set up monitoring for failed scraping jobs

## ğŸ“ Support

If you encounter issues:
1. Check the `scrape_logs` table for error details
2. Run health check: `npm run scrape:health`
3. Verify environment variables are set correctly
4. Check Chrome/Chromium installation
5. Review the comprehensive documentation in `scripts/README.md`

---

**ğŸ‰ This deployment ensures comprehensive schedule syncing for all companies, departments, and teams with enterprise-grade logging and monitoring!**