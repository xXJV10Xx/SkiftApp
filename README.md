# ğŸ”„ Skiftschema.se Scraper

Automatisk scraper fÃ¶r att klona alla skiftscheman frÃ¥n skiftschema.se och ladda upp till Supabase.

## ğŸ“‹ Ã–versikt

Detta projekt automatiserar processen att:
1. ğŸ•¸ï¸ Skrapa alla fÃ¶retag och lag frÃ¥n skiftschema.se
2. ğŸ—„ï¸ Generera SQL-schema fÃ¶r databas
3. ğŸ“… Extrahera skiftdata frÃ¥n varje lags kalender
4. â˜ï¸ Ladda upp data till Supabase

## ğŸš€ Snabbstart

### 1. Installation

```bash
# Klona projektet
git clone <repository-url>
cd skiftschema-scraper

# Installera dependencies
npm install
```

### 2. Konfigurera Supabase

1. Skapa ett nytt projekt pÃ¥ [Supabase](https://supabase.com)
2. Kopiera din projekt-URL och API-nyckel
3. Uppdatera `.env`-filen:

```env
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_KEY=your-supabase-anon-key
```

### 3. KÃ¶r scrapern

```bash
# KÃ¶r komplett scraping och upload
npm start

# Eller kÃ¶r steg fÃ¶r steg:
npm run scrape-teams    # Skrapa alla lag frÃ¥n webbplatsen
npm run scrape-upload   # Ladda upp skiftdata till Supabase
```

## ğŸ“Š Data Structure

### Companies (FÃ¶retag)
- ABB, SSAB, LKAB, Stora Enso, Barilla, etc.

### Departments (Avdelningar)
- HVC, BorlÃ¤nge, Kiruna, NymÃ¶lla, Filipstad, etc.

### Teams (Lag)
- A-skift, B-skift, Lag 1, Grupp 1, etc.

### Shifts (Skift)
- **F** (FÃ¶rmiddag): 06:00-14:00
- **E** (Eftermiddag): 14:00-22:00  
- **N** (Natt): 22:00-06:00
- **L** (Ledigt): Ingen arbetstid

## ğŸ—„ï¸ Databas Schema

KÃ¶r SQL-skriptet fÃ¶r att skapa tabellerna:

```sql
-- Se skiftschema-schema.sql fÃ¶r komplett schema
CREATE TABLE companies (id, name, created_at);
CREATE TABLE departments (id, company_id, name, created_at);
CREATE TABLE teams (id, department_id, name, url, created_at);
CREATE TABLE shifts (id, team_id, date, shift_type, start_time, end_time, raw_data, created_at, updated_at);
```

## ğŸ“ Filstruktur

```
skiftschema-scraper/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ scrape-upload-cursor.cjs    # Huvudsaklig scraper
â”œâ”€â”€ teams-array.js                  # Alla lag och URL:er
â”œâ”€â”€ skiftschema-schema.sql          # Databas schema
â”œâ”€â”€ scrape-skiftschema.js          # Initial scraper fÃ¶r lag
â”œâ”€â”€ package.json                    # Dependencies
â”œâ”€â”€ .env                           # Supabase konfiguration
â””â”€â”€ README.md                      # Denna fil
```

## ğŸ”§ Konfiguration

### Environment Variables (.env)
```env
# Supabase Configuration
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_KEY=your-supabase-anon-key

# Scraper Configuration  
HEADLESS_BROWSER=true
BATCH_SIZE=3
DELAY_BETWEEN_BATCHES=2000
```

### Teams Array (teams-array.js)
InnehÃ¥ller alla 75+ lag frÃ¥n 12 fÃ¶retag:

```javascript
const TEAMS = [
  { id: 1, company: "ABB", department: "HVC", team: "Skift 1", url: "https://..." },
  { id: 2, company: "ABB", department: "HVC", team: "Skift 2", url: "https://..." },
  // ... 73 fler lag
];
```

## âš™ï¸ Avancerad anvÃ¤ndning

### Skrapa specifika lag

```javascript
const { scrapeTeamSchedule } = require('./scripts/scrape-upload-cursor.cjs');

// Skrapa ett specifikt lag
const team = { id: 1, company: "ABB", team: "Skift 1", url: "https://..." };
const shifts = await scrapeTeamSchedule(team);
console.table(shifts);
```

### Anpassad batch-storlek

```javascript
// I scripts/scrape-upload-cursor.cjs
const BATCH_SIZE = 5; // Ã„ndra frÃ¥n 3 till 5 lag per batch
```

## ğŸ“ˆ Statistik

Scrapern hanterar:
- **12 fÃ¶retag** (ABB, SSAB, LKAB, Stora Enso, etc.)
- **15 avdelningar** (olika orter och fabriker)
- **75 lag** (olika skift och grupper)
- **~27,000 skiftposter** per Ã¥r (365 dagar Ã— 75 lag)

## ğŸ› ï¸ FelsÃ¶kning

### Vanliga problem

**Puppeteer timeout:**
```bash
# Ã–ka timeout i scraper-filen
await page.goto(team.url, { timeout: 60000 });
```

**Supabase connection error:**
```bash
# Kontrollera .env-filen
echo $SUPABASE_URL
echo $SUPABASE_KEY
```

**Ingen data extraherad:**
```bash
# KÃ¶r i non-headless mode fÃ¶r debugging
const browser = await puppeteer.launch({ headless: false });
```

### Debug mode

```javascript
// I scripts/scrape-upload-cursor.cjs
const browser = await puppeteer.launch({ 
  headless: false,  // Visa webblÃ¤saren
  slowMo: 250      // LÃ¥ngsammare fÃ¶r debugging
});
```

## ğŸ“… Automatisering

### Cron job (Linux/Mac)
```bash
# KÃ¶r varje dag kl 06:00
0 6 * * * cd /path/to/skiftschema-scraper && npm run scrape-upload
```

### GitHub Actions
```yaml
name: Daily Scrape
on:
  schedule:
    - cron: '0 6 * * *'  # Varje dag 06:00 UTC
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: npm install
      - run: npm run scrape-upload
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
```

## ğŸ¤ Bidra

1. Forka projektet
2. Skapa en feature branch (`git checkout -b feature/amazing-feature`)
3. Commita dina Ã¤ndringar (`git commit -m 'Add amazing feature'`)
4. Pusha till branchen (`git push origin feature/amazing-feature`)
5. Ã–ppna en Pull Request

## ğŸ“„ Licens

MIT License - se LICENSE-filen fÃ¶r detaljer.

## âš–ï¸ Ansvarsskyldighet

Denna scraper Ã¤r endast fÃ¶r utbildningssyfte. Respektera skiftschema.se:s terms of service och anvÃ¤nd inte scrapern fÃ¶r kommersiella Ã¤ndamÃ¥l utan tillstÃ¥nd.

---

**Made with â¤ï¸ by Cursor Agent**
